{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from tsne_torch import TorchTSNE as TSNE\n","import lightgbm as lgbm\n","import os\n","import pandas as pd\n","import pywt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm\n","from torch.utils.data import DataLoader\n","from tqdm.notebook import tqdm\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","#torch.backends.cudnn.benchmark = True\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Temporal Convolution Network\n"," The following classes implement the TCN as explained in the paper [\"Temporal Convolutional Networks: A Unified Approach to Action Segmentation\"](https://link.springer.com/chapter/10.1007/978-3-319-49409-8_7)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","\n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()\n","\n","\n","class TemporalBlock(nn.Module):\n","    def __init__(\n","            self,\n","            n_inputs,\n","            n_outputs,\n","            kernel_size,\n","            stride,\n","            dilation,\n","            padding,\n","            dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = weight_norm(\n","            nn.Conv1d(\n","                n_inputs,\n","                n_outputs,\n","                kernel_size,\n","                stride=stride,\n","                padding=padding,\n","                dilation=dilation))\n","        self.chomp1 = Chomp1d(padding)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","\n","        self.conv2 = weight_norm(\n","            nn.Conv1d(\n","                n_outputs,\n","                n_outputs,\n","                kernel_size,\n","                stride=stride,\n","                padding=padding,\n","                dilation=dilation))\n","        self.chomp2 = Chomp1d(padding)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","        self.net = nn.Sequential(\n","            self.conv1,\n","            self.chomp1,\n","            self.relu1,\n","            self.dropout1,\n","            self.conv2,\n","            self.chomp2,\n","            self.relu2,\n","            self.dropout2)\n","\n","        self.downsample = nn.Conv1d(\n","            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        nn.init.xavier_uniform(self.conv1.weight)\n","        nn.init.xavier_uniform(self.conv2.weight)\n","        if self.downsample is not None:\n","            nn.init.xavier_uniform(self.downsample.weight)\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class TemporalConvNet(nn.Module):\n","    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n","        super(TemporalConvNet, self).__init__()\n","        layers = []\n","        num_levels = len(num_channels)\n","        for i in range(num_levels):\n","            dilation_size = 2 ** i\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","            layers += [TemporalBlock(in_channels,\n","                                     out_channels,\n","                                     kernel_size,\n","                                     stride=1,\n","                                     dilation=dilation_size,\n","                                     padding=(kernel_size-1) * dilation_size,\n","                                     dropout=dropout)]\n","\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        # the data should have  dimension (N,C,L) where N is the batch size,\n","        # C is the number of channels and L the in input_length\n","        # this is the same as input dims for the nn.Conv1d\n","        return self.network(x)\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Haar Wavelet Transorm\n","The following two blocks implement the second part of the Driver2Vec architecture, related to the Haar wavelet transorm. Its aim is to capture spectral components of the inputs."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def reference_transform(tensor):\n","    \"\"\"\n","    Apply the Haar wavelet transform to a tensor\n","    \n","    input:\n","        tensor: a tensor with dimension (N, C, L), with N batch size, C number of channels and L the input length\n","    \n","    output:\n","        a tensor with dimensions (N, C, L) where the the two output channels of the transform are concatenated along the L dimension\n","    \"\"\"\n","    array = tensor.numpy()\n","    out1, out2 = pywt.dwt(array, \"haar\")\n","    out1 = torch.from_numpy(out1)\n","    out2 = torch.from_numpy(out2)\n","\n","    # concatenate each channel to be able to concatenate it to the untransformed data\n","    # everything will then be split when fed to the network\n","    return torch.cat((out1, out2), -1)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class WaveletPart(nn.Module):\n","    \"\"\"\n","    Module to map the (N, C, L) output of the Haar transform to a (N, 2*O) tensor\n","    \"\"\"\n","\n","    def __init__(self, input_length, input_size, output_size):\n","        \"\"\"\n","        inputs:\n","            input_length: length of the initial sequence fed to the network\n","            input_size: size of the inputs of the FC layer \n","            output_size: output size of the FC layer\n","        \"\"\"\n","        super(WaveletPart, self).__init__()\n","\n","        # used two different layers here as in the paper but in the github code, they are the same\n","        self.fc1 = nn.Linear(input_size, output_size)\n","        self.fc2 = nn.Linear(input_size, output_size)\n","\n","        self.input_size = input_size\n","        self.input_length = input_length\n","\n","        self.haar = reference_transform\n","\n","    def init_weight(self):\n","        self.fc1.weight.data.normal_(0, 0.01)\n","        self.fc1.bias.data.normal_(0, 0.01)\n","        self.fc2.weight.data.normal_(0, 0.01)\n","        self.fc2.bias.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        # split the wavelet transformed data along third dim\n","        # the data should have  dimension (N,C,L*2) where N is the batch size,\n","        # C is the number of channels and L the in input_length (*2 because of wavelet transforma concatenation)\n","        x1, x2 = torch.split(x, self.input_length//2, 2)\n","\n","        # reshape everything to feed to the linear layer\n","        bsize = x.size()[0]\n","        x1 = self.fc1(x1.reshape((bsize, -1, 1)).squeeze())\n","        x2 = self.fc2(x2.reshape((bsize, -1, 1)).squeeze())\n","        x1 = x1.reshape(bsize, -1)\n","        x2 = x2.reshape(bsize, -1)\n","        return torch.cat((x1, x2), -1)\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Full architecture\n","The following class implements the full architecture of Driver2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class Driver2Vec(nn.Module):\n","    def __init__(\n","            self,\n","            input_size,\n","            input_length,\n","            num_channels,\n","            output_size,\n","            kernel_size,\n","            dropout,\n","            do_wavelet=True,\n","            fc_output_size=15):\n","        super(Driver2Vec, self).__init__()\n","\n","        self.tcn = TemporalConvNet(input_size,\n","                                   num_channels,\n","                                   kernel_size=kernel_size,\n","                                   dropout=dropout)\n","        self.wavelet = do_wavelet\n","        if self.wavelet:\n","            self.haar = WaveletPart(\n","                input_length, input_size*input_length//2, fc_output_size)\n","\n","            linear_size = num_channels[-1] + fc_output_size*2\n","        else:\n","            linear_size = num_channels[-1]\n","        self.input_length = input_length\n","\n","        self.input_bn = nn.BatchNorm1d(linear_size)\n","        self.linear = nn.Linear(linear_size, output_size)\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, inputs, print_temp=False):\n","        \"\"\"Inputs have to have dimension (N, C_in, L_in*2)\n","        the base time series, and the two wavelet transform channel are concatenated along the third dim\"\"\"\n","\n","        # split the inputs, in the last dim, first is the unchanged data, then\n","        # the wavelet transformed data\n","        input_tcn, input_haar = torch.split(inputs, self.input_length, 2)\n","\n","        # feed each one to their corresponding network\n","        y1 = self.tcn(input_tcn)\n","        # for the TCN, only the last output element interests us\n","        y1 = y1[:, :, -1]\n","\n","        if self.wavelet:\n","            y2 = self.haar(input_haar)\n","\n","            out = torch.cat((y1, y2), 1)\n","        else:\n","            out = y1\n","        bsize = out.shape[0]\n","\n","        if bsize > 1:  # issue when the batch size is 1, can't batch normalize it\n","            out = self.input_bn(out)\n","        else:\n","            out = out\n","        out = self.linear(out)\n","        out = self.activation(out)\n","\n","        if print_temp:\n","            print(out)\n","\n","        return out\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Hard Triplet Loss\n"," The following class implements the hard triplet loss. Hard means that the closest negative and furthest positive are choosen instead of random."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class HardTripletLoss():\n","\n","    def __init__(self, device, margin=1.0):\n","        self.margin = margin\n","        self.device = device\n","\n","    def _get_anchor_positive_triplet_mask(self, labels):\n","        labels_mat = labels.unsqueeze(0).repeat(labels.shape[0], 1)\n","        res = (labels_mat == labels_mat.T).int().to(self.device)\n","        return res\n","\n","    def _get_anchor_negative_triplet_mask(self, labels):\n","        labels_mat = labels.unsqueeze(0).repeat(labels.shape[0], 1)\n","        res = (labels_mat != labels_mat.T).int().to(self.device)\n","        return res\n","\n","    def _get_dist_matrix(self, embeddings):\n","        return torch.cdist(embeddings, embeddings).to(self.device)\n","\n","    def __call__(self, embeddings, labels):\n","        \"\"\"Build the triplet loss over a batch of embeddings.\n","\n","        For each anchor, we get the hardest positive and hardest negative to form a triplet.\n","\n","        Args:\n","            labels: labels of the batch, of size (batch_size,)\n","            embeddings: tensor of shape (batch_size, embed_dim)\n","\n","        Returns:\n","            triplet_loss: scalar tensor containing the triplet loss\n","        \"\"\"\n","        # Get the pairwise distance matrix\n","        pairwise_dist = self._get_dist_matrix(embeddings)\n","\n","        # For each anchor, get the hardest positive\n","        # First, we need to get a mask for every valid positive (they should have same label)\n","        mask_anchor_positive = self._get_anchor_positive_triplet_mask(labels)\n","\n","        # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n","        anchor_positive_dist = mask_anchor_positive * pairwise_dist\n","\n","        # shape (batch_size, 1)\n","        hardest_positive_dist, _ = torch.max(\n","            anchor_positive_dist, axis=1, keepdims=True)\n","\n","        # For each anchor, get the hardest negative\n","        # First, we need to get a mask for every valid negative (they should have different labels)\n","        mask_anchor_negative = self._get_anchor_negative_triplet_mask(labels)\n","\n","        # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n","        max_anchor_negative_dist, _ = torch.max(\n","            pairwise_dist, axis=1, keepdims=True)\n","        anchor_negative_dist = pairwise_dist + \\\n","            max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n","\n","        # shape (batch_size,)\n","        hardest_negative_dist, _ = torch.min(\n","            anchor_negative_dist, axis=1, keepdims=True)\n","\n","        # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n","        triplet_loss = torch.max(hardest_positive_dist -\n","                                 hardest_negative_dist + self.margin, torch.zeros_like(hardest_negative_dist))\n","\n","        # Get final mean triplet loss\n","        triplet_loss = torch.mean(triplet_loss)\n","\n","        return triplet_loss\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Datasets\n"," The following 3 classes are used to handle the dataset that we have."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def preprocess(df):\n","    return (\n","        df.drop(\n","            [\"FOG\",\n","             \"FOG_LIGHTS\",\n","             \"FRONT_WIPERS\",\n","             \"HEAD_LIGHTS\",\n","             \"RAIN\",\n","             \"REAR_WIPERS\",\n","             \"SNOW\",\n","             ], axis=1\n","        )\n","    )\n","\n","\n","class TrainDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    This class is used to handle the train dataset to work witht the Triplet Loss.\n","    The __getitem__ method (to be used with a dataloader) returns the anchor, a random postive and a random negative for that anchor\n","    as well as the anchor's label.\n","    \"\"\"\n","\n","    def __init__(self, data, labels, input_length=500):\n","        self.data, self.labels = data, labels\n","        self.index = [i for i in range(len(self.data))]\n","        self.length = input_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        max_length = self.data[index].shape[1]\n","        # int(np.random.uniform(0, max_length - self.length+1))\n","        start_pos_anchor = int(np.random.uniform(0, max_length - self.length+1))\n","        X_anchor = self.data[index][:,\n","                                    start_pos_anchor:start_pos_anchor+self.length]\n","        anchor_wvlt = reference_transform(X_anchor)\n","        y_anchor = self.labels[index]\n","\n","        # concatenate the data for the TCN and the haar wavelet transform\n","        # they will be split in the forward pass\n","        return torch.cat((X_anchor, anchor_wvlt), 1), \\\n","            y_anchor\n","\n","\n","class TestDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Is to be used with Dataloader for testing only\n","    \"\"\"\n","\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        wvlt = reference_transform(x)\n","        out = torch.cat((x, wvlt), 1)\n","        return out, self.labels[index]\n","\n","\n","class FromFiles:\n","    \"\"\"\n","    Used to load the data from the files and split between test and train sets\n","    \"\"\"\n","\n","    def __init__(self, input_dir, input_length):\n","        self.input_dir = input_dir\n","        self.input_length = input_length\n","\n","        self.data, self.labels = self.__load_dataset()\n","        self.index = [i for i in range(len(self.data))]\n","\n","    def __load_dataset(self):\n","        x = []\n","        y = []\n","        for dir, _, files in os.walk(self.input_dir):\n","            for file in files:\n","                label = int(file.split(\"_\")[1])-1\n","                df = pd.read_csv(dir + \"/\" + file, index_col=0)\n","                df = preprocess(df).to_numpy().transpose()\n","                x.append(torch.from_numpy(df).float())\n","                y.append(label)\n","        return x, y\n","\n","    def split_train_test(self):\n","        x_test, y_test = [], []\n","        for label in range(5):\n","            possible_list = [i for i in self.index if self.labels[i] == label]\n","            choosen_index = np.random.choice(possible_list)\n","\n","            positive = self.data[choosen_index]\n","            max_length = positive.shape[1]\n","            train, test = torch.split(\n","                positive, [max_length-self.input_length, self.input_length], dim=1)\n","\n","            x_test.append(test)\n","            y_test.append(label)\n","            self.data[choosen_index] = train\n","\n","        return self.data, self.labels, x_test, y_test\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Custom Dataloader\n"," This custom dataset is useful to load bigger batches than the 19 sample sequences that we have."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Dataloader():\n","    \"\"\"Homemade dataloader for our needs in training\n","    This is different from the other one as it allows for \"infinite\" batches, even when the data only has \n","    19 points. When setting bacht_size*number_batches bigger that the total number of points in the dataset,\n","    this dataloader will just loop again from the beginning.\"\"\"\n","\n","    def __init__(self, dataset: TrainDataset, batch_size: int, number_batch: int, shuffle=True):\n","        self.dataset = dataset\n","        self.b_size = batch_size\n","        self.n_batches = number_batch\n","        self.current_batch = 0\n","        self.shuffle = shuffle\n","\n","    def __iter__(self):\n","        self.current_batch = 0\n","        self.index_list = [i % len(self.dataset) for i in range(\n","            max(self.b_size, len(self.dataset)))]\n","        if self.shuffle:\n","            np.random.shuffle(self.index_list)\n","        self.i = -1\n","        return self\n","\n","    def __next__(self):\n","        dataset_length = len(self.dataset)\n","        if self.current_batch < dataset_length:\n","            a_out, l_out = [], []\n","            for _ in range(self.b_size):\n","                self.i = (self.i+1) % dataset_length\n","                a, l = self.dataset[self.index_list[self.i]]\n","                a_out.append(a)\n","                l_out.append(l)\n","            self.current_batch += 1\n","            a_out = torch.stack(a_out)\n","            l_out = torch.Tensor(l_out)\n","            return a_out, l_out\n","\n","        else:\n","            self.current_batch = 0\n","            raise StopIteration\n",""]},{"cell_type":"markdown","metadata":{},"source":["# The model\n","The following code is the Driver2Vec model setup."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_channels = 31\n","input_length = 300\n","channel_sizes = [25,25]\n","output_size = 62\n","kernel_size = 16\n","dropout = 0.1\n","model = Driver2Vec(input_channels, input_length, channel_sizes, output_size,\n","                   kernel_size=kernel_size, dropout=dropout, do_wavelet=False)\n","model.to(device)\n",""]},{"cell_type":"markdown","metadata":{},"source":["Next are the dataloader, loss and optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# # datasets parameters\n","# params = {'batch_size': 4,\n","#           'shuffle': True,\n","#           'num_workers': 1}\n","\n","\n","fromfiles = FromFiles(\"./dataset\", input_length)\n","x_train, y_train, x_test, y_text = fromfiles.split_train_test()\n","training_set = TrainDataset(x_train, y_train, input_length)\n","training_generator = Dataloader(training_set, 20, 4)\n","\n","loss = HardTripletLoss(device, margin=0.5)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.004, weight_decay=0.0001)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Finally, here is the training loop."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epochs = 50\n","\n","\n","model.train()\n","for epoch in (pbar := tqdm(range(epochs))):\n","    loss_list = []\n","    for anchor, label in training_generator:\n","        anchor = anchor.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        y_anchor = model(anchor)\n","\n","        loss_value = loss(y_anchor, label)\n","        loss_value.backward()\n","\n","        optimizer.step()\n","\n","        loss_list.append(loss_value.cpu().detach().numpy())\n","    #print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(loss_list)))\n","    pbar.set_description(\"Loss: %0.5g, Epochs\" % (np.mean(loss_list)))\n",""]},{"cell_type":"markdown","metadata":{},"source":["# LightGBM classifier\n","The following is the setup and the training of the LightGBM classifier."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","params1 = {'batch_size': 19,\n","           'shuffle': False,\n","           'num_workers': 2}\n","\n","params2 = {'batch_size': 38,\n","           'shuffle': False,\n","           'number_batch': 1}\n","\n","classifier_train_set = TrainDataset(x_test, y_text, input_length)\n","classifier_train_generator = DataLoader(training_set, **params1)\n","\n","x_train_classifier = []\n","y_train_classifier = []\n","model.train(False)\n","for data, label in classifier_train_generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","\n","for i in range(5):\n","    x_train_classifier.append(embed[i, :].cpu().detach().numpy().squeeze())\n","    y_train_classifier.append(int(label[i]))\n","\n","x_train_classifier = np.array(x_train_classifier)\n","y_train_classifier = np.array(y_train_classifier)\n","\n","lgb_train = lgbm.Dataset(x_train_classifier, y_train_classifier)\n","\n","params = {\n","    'boosting_type': 'gbdt',\n","    'objective': 'multiclass',\n","    'num_class': 5,\n","    'metric': 'multi_logloss',\n","    'num_leaves': 32,\n","    'feature_fraction': 0.8,\n","    'bagging_fraction': 0.9,\n","    'max_depth': 8,\n","    'num_trees': 30,\n","    'verbose': 0,\n","    'min_data_in_leaf': 2  # May need to change that with a real test set\n","}\n","\n","clf = lgbm.train(params, lgb_train)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Testing the classifier on the test set we made and also a part of the training set to evaluate it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["params = {'batch_size': 5,\n","          'shuffle': False,\n","          'num_workers': 1}\n","\n","classifier_test_set = TestDataset(x_test, y_text)\n","classifier_test_generator = DataLoader(classifier_test_set, **params)\n","\n","x_test_classifier = []\n","y_test_classifier = []\n","for data, label in classifier_test_generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","\n","for i in range(5):\n","    x_test_classifier.append(embed[i, :].cpu().detach().numpy().squeeze())\n","    y_test_classifier.append(int(label[i]))\n","\n","\n","y_pred = clf.predict(x_test_classifier)\n","print(y_pred, y_test_classifier)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def distance_matrix(tensor: torch.Tensor):\n","    \"\"\"\n","    computes the distance matrix between each point of the input Tensor\n","    tensor should have dimension (L,D) where D is the dimension of the vectors\"\"\"\n","\n","    res = torch.cdist(tensor, tensor)\n","    return res\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["params = {\"batch_size\": 19,\n","          \"shuffle\": False,\n","          \"num_workers\": 1}\n","\n","generator = DataLoader(training_set, **params)\n","x_test_classifier = []\n","y_test_classifier = []\n","for data, label in generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","\n","for i in range(19):\n","    x_test_classifier.append(embed[i, :].cpu().detach().numpy().squeeze())\n","    y_test_classifier.append(int(label[i]))\n","\n","\n","y_pred = clf.predict(x_test_classifier)\n","print(y_pred, y_test_classifier)\n",""]},{"cell_type":"markdown","metadata":{},"source":["# T-SNE visualisation\n"," As in the original paper, we us t-SNE to visualise the embeddings. It can help to spot the issues in the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","params = {'batch_size': 60,\n","          'shuffle': False,\n","          'number_batch': 1}\n","\n","generator = Dataloader(training_set, **params)\n","x_tsne = []\n","y_tsne = []\n","for data, label in generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","print(embed.shape)\n","\n","for i in range(embed.shape[0]):\n","    x_tsne.append(embed[i, :].cpu().detach().numpy().squeeze())\n","    y_tsne.append(int(label[i]))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","X_emb = TSNE(n_components=2, perplexity=60, n_iter=10000,\n","             verbose=True).fit_transform(embed)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.scatter(X_emb[:, 0], X_emb[:, 1], marker=\"+\", c=y_tsne)\n","plt.show()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}
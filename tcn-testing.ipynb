{"cells":[{"cell_type":"code","execution_count":817,"id":"0461b0b5-830e-423e-89e8-f13be8cbb5ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647519373490,"user":{"displayName":"Achille Bailly","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08818389123052568032"},"user_tz":-60},"id":"0461b0b5-830e-423e-89e8-f13be8cbb5ca","outputId":"73fca627-0d50-47a1-8f2a-244db0747f54"},"outputs":[],"source":["import pandas as pd\n","import pywt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm\n","from torch.utils.data import DataLoader\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","#torch.backends.cudnn.benchmark = True"]},{"cell_type":"markdown","id":"2b9fdf22-4599-4c24-99f6-0157b67c183c","metadata":{"id":"2b9fdf22-4599-4c24-99f6-0157b67c183c","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## Wavelet transform"]},{"cell_type":"code","execution_count":818,"id":"8aab9c59-2b06-4377-b613-33aa26226589","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1647520697786,"user":{"displayName":"Achille Bailly","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08818389123052568032"},"user_tz":-60},"id":"8aab9c59-2b06-4377-b613-33aa26226589","outputId":"18bfd94a-a711-4ed4-ba0c-85831c6692ef"},"outputs":[],"source":["def van_haar(data):\n","    ncol = data.shape[1]\n","    nrow = data.shape[0]\n","    for i in range(ncol):\n","        cur_col = data[:,i].copy()\n","        (cA, cD) = pywt.dwt(cur_col, 'haar')\n","        new_col = np.reshape(np.concatenate((cA,cD), 0),(nrow,1))\n","        data = np.hstack((data,new_col))\n","    data = data.reshape(nrow,-1)\n","    return data\n","\n","#van_haar(test.to_numpy()).shape"]},{"cell_type":"markdown","id":"0160ea70-e4e5-4800-93f3-f12256429544","metadata":{"id":"0160ea70-e4e5-4800-93f3-f12256429544"},"source":["## PyTorch"]},{"cell_type":"code","execution_count":819,"id":"84904e04-a87e-4229-a049-5c93e8bfe4b1","metadata":{"id":"84904e04-a87e-4229-a049-5c93e8bfe4b1"},"outputs":[],"source":["class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","\n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()\n","\n","class TemporalBlock(nn.Module):\n","    def __init__(\n","            self, \n","            n_inputs, \n","            n_outputs, \n","            kernel_size, \n","            stride,\n","            dilation,\n","            padding,\n","            dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = weight_norm(\n","            nn.Conv1d(\n","                n_inputs, \n","                n_outputs, \n","                kernel_size,\n","                stride=stride, \n","                padding=padding,\n","                dilation=dilation))\n","        self.pad = Chomp1d(padding)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.conv2 = weight_norm(\n","            nn.Conv1d(\n","                n_outputs, \n","                n_outputs, \n","                kernel_size,\n","                stride=stride, \n","                padding=padding, \n","                dilation=dilation))\n","        self.net = nn.Sequential(\n","            self.pad, \n","            self.conv1, \n","            self.relu, \n","            self.dropout,\n","            self.pad, \n","            self.conv2, \n","            self.relu, \n","            self.dropout)\n","        \n","        self.downsample = nn.Conv1d(\n","            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.conv1.weight.data.normal_(0, 0.01)\n","        self.conv2.weight.data.normal_(0, 0.01)\n","        if self.downsample is not None:\n","            self.downsample.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class TemporalConvNet(nn.Module):\n","    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n","        super(TemporalConvNet, self).__init__()\n","        layers = []\n","        num_levels = len(num_channels)\n","        for i in range(num_levels):\n","            dilation_size = 2 ** i\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","            layers += [TemporalBlock(in_channels, \n","                                     out_channels, \n","                                     kernel_size, \n","                                     stride=1, \n","                                     dilation=dilation_size,\n","                                     padding=(kernel_size-1) * dilation_size, \n","                                     dropout=dropout)]\n","\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.network(x)"]},{"cell_type":"code","execution_count":820,"id":"d37282df","metadata":{},"outputs":[],"source":["def reference_transform(tensor):\n","    array = tensor.numpy()\n","    out1, out2= pywt.dwt(array, \"haar\")\n","    out1 = torch.from_numpy(out1)\n","    out2 = torch.from_numpy(out2)\n","    \n","    #concatenate each channel to be able to concatenate it to the untransformed data\n","    #everything will then be split when fed to the network\n","    return torch.cat((out1, out2),-1)\n","    "]},{"cell_type":"code","execution_count":821,"id":"35ed31f1","metadata":{},"outputs":[],"source":["class WaveletPart(nn.Module):\n","\n","    def __init__(self, input_size, output_size):\n","        super(WaveletPart, self).__init__()\n","\n","        # used two different layers here as in the paper but in the github code, they are the same\n","        self.fc1 = nn.Linear(input_size, output_size)\n","        self.fc2 = nn.Linear(input_size, output_size)\n","\n","        self.input_size = input_size\n","\n","        self.haar = reference_transform\n","\n","    def init_weight(self):\n","        self.fc1.weight.data.normal_(0, 0.01)\n","        self.fc1.bias.data.normal_(0, 0.01)\n","        self.fc2.weight.data.normal_(0, 0.01)\n","        self.fc2.bias.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        # split the wavelet transformed data along third dim\n","        x1, x2 = torch.split(x, 500, 2)\n","\n","        # reshape everything to feed to the linear layer\n","        bsize = x.size()[0]\n","        x1 = self.fc1(x1.reshape((bsize, -1, 1)).squeeze())\n","        x2 = self.fc2(x2.reshape((bsize, -1, 1)).squeeze())\n","        x1 = x1.reshape(bsize, -1)\n","        x2 = x2.reshape(bsize, -1)\n","        return torch.cat((x1,x2),-1)"]},{"cell_type":"code","execution_count":822,"id":"b463b9dc-6eab-401e-a927-c7f00b3de9d0","metadata":{"id":"b463b9dc-6eab-401e-a927-c7f00b3de9d0"},"outputs":[],"source":["class Driver2Vec(nn.Module):\n","    def __init__(\n","            self, \n","            input_size, \n","            input_length, \n","            num_channels,\n","            output_size, \n","            kernel_size, \n","            dropout,\n","            fc_output_size = 15):\n","        super(Driver2Vec, self).__init__()\n","        \n","        self.tcn = TemporalConvNet(input_size, \n","                                   num_channels, \n","                                   kernel_size=kernel_size, \n","                                   dropout=dropout)\n","        \n","        self.haar = WaveletPart(input_size*input_length//2, fc_output_size)\n","\n","        linear_size = num_channels[-1] + fc_output_size*2\n","        self.input_length = input_length\n","\n","        self.input_bn = nn.BatchNorm1d(linear_size)\n","        self.linear = nn.Linear(linear_size, output_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Inputs have to have dimension (N, C_in, L_in*2)\n","        the base time series, and the two wavelet transform channel are concatenated along dim2\"\"\"\n","        \n","        # split the inputs, in the last dim, first is the unchanged data, then \n","        # the wavelet transformed data\n","        input_tcn, input_haar = torch.split(inputs, self.input_length, 2)\n","\n","        # feed each one to their corresponding network\n","        y1 = self.tcn(input_tcn)\n","        y1 = y1[:,:,-1] # for the TCN, only the last output element interests us\n","        y2 = self.haar(input_haar)\n","\n","        concat = torch.cat((y1,y2),1)\n","        bsize = concat.shape[0]\n","        if bsize > 1: # issue when the batch size is 1, can't batch normalize it\n","            out = self.input_bn(concat)\n","        else:\n","            out = concat\n","        out = self.linear(out)\n","        \n","        return out"]},{"cell_type":"code","execution_count":823,"id":"40dedd1a-7fcf-43d9-ac63-fd5810e2f90c","metadata":{"id":"40dedd1a-7fcf-43d9-ac63-fd5810e2f90c"},"outputs":[{"data":{"text/plain":["Driver2Vec(\n","  (tcn): TemporalConvNet(\n","    (network): Sequential(\n","      (0): TemporalBlock(\n","        (conv1): Conv1d(31, 25, kernel_size=(16,), stride=(1,), padding=(15,))\n","        (pad): Chomp1d()\n","        (relu): ReLU()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (conv2): Conv1d(25, 25, kernel_size=(16,), stride=(1,), padding=(15,))\n","        (net): Sequential(\n","          (0): Chomp1d()\n","          (1): Conv1d(31, 25, kernel_size=(16,), stride=(1,), padding=(15,))\n","          (2): ReLU()\n","          (3): Dropout(p=0.1, inplace=False)\n","          (4): Chomp1d()\n","          (5): Conv1d(25, 25, kernel_size=(16,), stride=(1,), padding=(15,))\n","          (6): ReLU()\n","          (7): Dropout(p=0.1, inplace=False)\n","        )\n","        (downsample): Conv1d(31, 25, kernel_size=(1,), stride=(1,))\n","      )\n","      (1): TemporalBlock(\n","        (conv1): Conv1d(25, 25, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","        (pad): Chomp1d()\n","        (relu): ReLU()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (conv2): Conv1d(25, 25, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","        (net): Sequential(\n","          (0): Chomp1d()\n","          (1): Conv1d(25, 25, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","          (2): ReLU()\n","          (3): Dropout(p=0.1, inplace=False)\n","          (4): Chomp1d()\n","          (5): Conv1d(25, 25, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","          (6): ReLU()\n","          (7): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): TemporalBlock(\n","        (conv1): Conv1d(25, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n","        (pad): Chomp1d()\n","        (relu): ReLU()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n","        (net): Sequential(\n","          (0): Chomp1d()\n","          (1): Conv1d(25, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n","          (2): ReLU()\n","          (3): Dropout(p=0.1, inplace=False)\n","          (4): Chomp1d()\n","          (5): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n","          (6): ReLU()\n","          (7): Dropout(p=0.1, inplace=False)\n","        )\n","        (downsample): Conv1d(25, 32, kernel_size=(1,), stride=(1,))\n","      )\n","    )\n","  )\n","  (haar): WaveletPart(\n","    (fc1): Linear(in_features=15500, out_features=15, bias=True)\n","    (fc2): Linear(in_features=15500, out_features=15, bias=True)\n","  )\n","  (input_bn): BatchNorm1d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (linear): Linear(in_features=62, out_features=62, bias=True)\n",")"]},"execution_count":823,"metadata":{},"output_type":"execute_result"}],"source":["input_channels = 31\n","channel_sizes = [25,25,32]\n","output_size = 62\n","kernel_size = 16\n","dropout = 0.1\n","model = Driver2Vec(input_channels, 1000, channel_sizes, output_size, kernel_size=kernel_size, dropout=dropout)\n","model.to(device)"]},{"cell_type":"code","execution_count":824,"id":"643074c4-0c64-4a6d-b79f-b560db0d3754","metadata":{"id":"643074c4-0c64-4a6d-b79f-b560db0d3754"},"outputs":[],"source":["import os\n","\n","def preprocess(df):\n","    return (\n","        df.drop(\n","            [\"FOG\",\n","             \"FOG_LIGHTS\",\n","             \"FRONT_WIPERS\",\n","             \"HEAD_LIGHTS\",\n","             \"RAIN\",\n","             \"REAR_WIPERS\",\n","             \"SNOW\",\n","            ], axis=1\n","        )\n","    )\n","\n","def load_dataset(input_dir):\n","    x=[]\n","    y=[]\n","    for dir,_,files in os.walk(input_dir):\n","        for file in files:\n","            label = int(file.split(\"_\")[1])\n","            df = pd.read_csv(dir + \"/\" + file, index_col=0)\n","            df = preprocess(df).to_numpy().transpose()\n","            x.append(torch.from_numpy(df).float())\n","            y.append(label)\n","    return x,y\n","        \n","class Dataset(torch.utils.data.Dataset):\n","    'Characterizes a dataset for PyTorch'\n","    def __init__(self, input_dir, train=True):\n","        'Initialization'\n","        self.data, self.labels = load_dataset(input_dir)\n","        self.index = [i for i in range(len(self.data))]\n","        self.train = train\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        X_anchor = self.data[index]\n","        anchor_wvlt = reference_transform(X_anchor)\n","        y_anchor = self.labels[index]\n","\n","        if self.train:\n","            # create list of possible positive index samples\n","            positive_list = [\n","                i for i in self.index if self.labels[i] == y_anchor and i != index]\n","            positive_index = np.random.choice(positive_list)\n","            positive = self.data[positive_index]\n","            # get the wavelet transform of that sample (the two channels are concatenated along the last dim)\n","            positive_wvlt = reference_transform(positive)\n","            \n","            # same here\n","            negative_list = [\n","                i for i in self.index if self.labels[i] != y_anchor]\n","            negative_index = np.random.choice(negative_list)\n","            negative = self.data[negative_index]\n","            negative_wvlt = reference_transform(negative)\n","\n","            # concatenate the data for the TCN and the haar wavelet transform\n","            # they will be split in the forward pass\n","            return torch.cat((X_anchor, anchor_wvlt),1), \\\n","                torch.cat((positive, positive_wvlt),1), \\\n","                torch.cat((negative, negative_wvlt),1), \\\n","                y_anchor\n","\n","        return X_anchor, y_anchor\n","\n"]},{"cell_type":"code","execution_count":825,"id":"3692f2fb","metadata":{},"outputs":[],"source":["\n","# datasets parameters\n","params = {'batch_size': 3,\n","          'shuffle': False,\n","          'num_workers': 1}\n","\n","training_set = Dataset(\"./dataset\")\n","training_generator = DataLoader(training_set, **params)\n","\n","loss = nn.TripletMarginLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":826,"id":"ec33b00d","metadata":{},"outputs":[],"source":["for anchor, positive, negative, label in training_generator:\n","    anchor = anchor.to(device)\n","    positive = positive.to(device)\n","    negative = negative.to(device)\n","\n","    optimizer.zero_grad()\n","    y_anchor = model(anchor)\n","    y_positive = model(positive)\n","    y_negative = model(negative)\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"tcn-testing.ipynb","provenance":[{"file_id":"https://github.com/AchilleBailly/Driver2Vec/blob/kick-off/tcn-testing.ipynb","timestamp":1647516778114}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":5}

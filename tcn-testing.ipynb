{"cells":[{"cell_type":"code","execution_count":43,"id":"0461b0b5-830e-423e-89e8-f13be8cbb5ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647519373490,"user":{"displayName":"Achille Bailly","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08818389123052568032"},"user_tz":-60},"id":"0461b0b5-830e-423e-89e8-f13be8cbb5ca","outputId":"73fca627-0d50-47a1-8f2a-244db0747f54"},"outputs":[],"source":["import pandas as pd\n","import pywt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm\n","from torch.utils.data import DataLoader\n","from tqdm.notebook import tqdm\n","\n","from triplet_loss import batch_hard_triplet_loss\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","#torch.backends.cudnn.benchmark = True"]},{"cell_type":"markdown","id":"2b9fdf22-4599-4c24-99f6-0157b67c183c","metadata":{"id":"2b9fdf22-4599-4c24-99f6-0157b67c183c","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## Wavelet transform"]},{"cell_type":"code","execution_count":44,"id":"8aab9c59-2b06-4377-b613-33aa26226589","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1647520697786,"user":{"displayName":"Achille Bailly","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08818389123052568032"},"user_tz":-60},"id":"8aab9c59-2b06-4377-b613-33aa26226589","outputId":"18bfd94a-a711-4ed4-ba0c-85831c6692ef"},"outputs":[],"source":["def van_haar(data):\n","    ncol = data.shape[1]\n","    nrow = data.shape[0]\n","    for i in range(ncol):\n","        cur_col = data[:,i].copy()\n","        (cA, cD) = pywt.dwt(cur_col, 'haar')\n","        new_col = np.reshape(np.concatenate((cA,cD), 0),(nrow,1))\n","        data = np.hstack((data,new_col))\n","    data = data.reshape(nrow,-1)\n","    return data\n","\n","#van_haar(test.to_numpy()).shape"]},{"cell_type":"markdown","id":"0160ea70-e4e5-4800-93f3-f12256429544","metadata":{"id":"0160ea70-e4e5-4800-93f3-f12256429544"},"source":["## PyTorch"]},{"cell_type":"code","execution_count":45,"id":"84904e04-a87e-4229-a049-5c93e8bfe4b1","metadata":{"id":"84904e04-a87e-4229-a049-5c93e8bfe4b1"},"outputs":[],"source":["class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","\n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()\n","\n","class TemporalBlock(nn.Module):\n","    def __init__(\n","            self, \n","            n_inputs, \n","            n_outputs, \n","            kernel_size, \n","            stride,\n","            dilation,\n","            padding,\n","            dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = weight_norm(\n","            nn.Conv1d(\n","                n_inputs,\n","                n_outputs,\n","                kernel_size,\n","                stride=stride,\n","                padding=padding,\n","                dilation=dilation))\n","        self.chomp1 = Chomp1d(padding)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","\n","        self.conv2 = weight_norm(\n","            nn.Conv1d(\n","                n_outputs,\n","                n_outputs,\n","                kernel_size,\n","                stride=stride,\n","                padding=padding,\n","                dilation=dilation))\n","        self.chomp2 = Chomp1d(padding)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","\n","        self.net = nn.Sequential(\n","            self.conv1,\n","            self.chomp1,\n","            self.relu1,\n","            self.dropout1,\n","            self.conv2,\n","            self.chomp2,\n","            self.relu2,\n","            self.dropout2)\n","        \n","        self.downsample = nn.Conv1d(\n","            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.conv1.weight.data.normal_(0, 0.01)\n","        self.conv2.weight.data.normal_(0, 0.01)\n","        if self.downsample is not None:\n","            self.downsample.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class TemporalConvNet(nn.Module):\n","    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n","        super(TemporalConvNet, self).__init__()\n","        layers = []\n","        num_levels = len(num_channels)\n","        for i in range(num_levels):\n","            dilation_size = 2 ** i\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","            layers += [TemporalBlock(in_channels, \n","                                     out_channels, \n","                                     kernel_size, \n","                                     stride=1, \n","                                     dilation=dilation_size,\n","                                     padding=(kernel_size-1) * dilation_size, \n","                                     dropout=dropout)]\n","\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        # the data should have  dimension (N,C,L) where N is the batch size, \n","        # C is the number of channels and L the in input_length\n","        # this is the same as input dims for the nn.Conv1d\n","        return self.network(x)"]},{"cell_type":"code","execution_count":46,"id":"d37282df","metadata":{},"outputs":[],"source":["def reference_transform(tensor):\n","    array = tensor.numpy()\n","    out1, out2= pywt.dwt(array, \"haar\")\n","    out1 = torch.from_numpy(out1)\n","    out2 = torch.from_numpy(out2)\n","    \n","    #concatenate each channel to be able to concatenate it to the untransformed data\n","    #everything will then be split when fed to the network\n","    return torch.cat((out1, out2),-1)\n","    "]},{"cell_type":"code","execution_count":47,"id":"35ed31f1","metadata":{},"outputs":[],"source":["class WaveletPart(nn.Module):\n","\n","    def __init__(self,input_length, input_size, output_size):\n","        super(WaveletPart, self).__init__()\n","\n","        # used two different layers here as in the paper but in the github code, they are the same\n","        self.fc1 = nn.Linear(input_size, output_size)\n","        self.fc2 = nn.Linear(input_size, output_size)\n","\n","        self.input_size = input_size\n","        self.input_length = input_length\n","\n","        self.haar = reference_transform\n","\n","    def init_weight(self):\n","        self.fc1.weight.data.normal_(0, 0.01)\n","        self.fc1.bias.data.normal_(0, 0.01)\n","        self.fc2.weight.data.normal_(0, 0.01)\n","        self.fc2.bias.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        # split the wavelet transformed data along third dim\n","        # the data should have  dimension (N,C,L*2) where N is the batch size, \n","        # C is the number of channels and L the in input_length (*2 because of wavelet transforma concatenation)\n","        x1, x2 = torch.split(x, self.input_length//2, 2)\n","\n","        # reshape everything to feed to the linear layer\n","        bsize = x.size()[0]\n","        x1 = self.fc1(x1.reshape((bsize, -1, 1)).squeeze())\n","        x2 = self.fc2(x2.reshape((bsize, -1, 1)).squeeze())\n","        x1 = x1.reshape(bsize, -1)\n","        x2 = x2.reshape(bsize, -1)\n","        return torch.cat((x1,x2),-1)"]},{"cell_type":"code","execution_count":48,"id":"b463b9dc-6eab-401e-a927-c7f00b3de9d0","metadata":{"id":"b463b9dc-6eab-401e-a927-c7f00b3de9d0"},"outputs":[],"source":["class Driver2Vec(nn.Module):\n","    def __init__(\n","            self, \n","            input_size, \n","            input_length, \n","            num_channels,\n","            output_size, \n","            kernel_size, \n","            dropout,\n","            do_wavelet = True,\n","            fc_output_size = 15):\n","        super(Driver2Vec, self).__init__()\n","        \n","        self.tcn = TemporalConvNet(input_size, \n","                                   num_channels, \n","                                   kernel_size=kernel_size, \n","                                   dropout=dropout)\n","        self.wavelet = do_wavelet\n","        if self.wavelet:\n","            self.haar = WaveletPart(input_length, input_size*input_length//2, fc_output_size)\n","\n","            linear_size = num_channels[-1] + fc_output_size*2\n","        else: \n","            linear_size = num_channels[-1]\n","        self.input_length = input_length\n","\n","        self.input_bn = nn.LayerNorm(linear_size)\n","        self.linear = nn.Linear( linear_size, output_size)\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, inputs, print_temp = False):\n","        \"\"\"Inputs have to have dimension (N, C_in, L_in*2)\n","        the base time series, and the two wavelet transform channel are concatenated along dim2\"\"\"\n","        \n","        # split the inputs, in the last dim, first is the unchanged data, then \n","        # the wavelet transformed data\n","        input_tcn, input_haar = torch.split(inputs, self.input_length, 2)\n","\n","        # feed each one to their corresponding network\n","        y1 = self.tcn(input_tcn)\n","        y1 = y1[:,:,-1] # for the TCN, only the last output element interests us\n","\n","        if self.wavelet:\n","            y2 = self.haar(input_haar)\n","\n","            out = torch.cat((y1,y2),1)\n","        else:\n","            out= y1\n","        bsize = out.shape[0]\n","\n","        if bsize > 1: # issue when the batch size is 1, can't batch normalize it\n","            out = self.input_bn(out)\n","        else:\n","            out = out\n","        out = self.linear(out)\n","        out = self.activation(out)\n","\n","        if print_temp:\n","            print(out)\n","        \n","        return out"]},{"cell_type":"code","execution_count":49,"id":"643074c4-0c64-4a6d-b79f-b560db0d3754","metadata":{"id":"643074c4-0c64-4a6d-b79f-b560db0d3754"},"outputs":[],"source":["import os\n","\n","def preprocess(df):\n","    return (\n","        df.drop(\n","            [\"FOG\",\n","             \"FOG_LIGHTS\",\n","             \"FRONT_WIPERS\",\n","             \"HEAD_LIGHTS\",\n","             \"RAIN\",\n","             \"REAR_WIPERS\",\n","             \"SNOW\",\n","            ], axis=1\n","        )\n","    )\n","\n","\n","\n","class TrainDataset(torch.utils.data.Dataset):\n","    'Characterizes a dataset for PyTorch'\n","    def __init__(self, data, labels, input_length=500):\n","        'Initialization'\n","        self.data, self.labels = data, labels\n","        self.index = [i for i in range(len(self.data))]\n","        self.length = input_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        max_length = self.data[index].shape[1]\n","        start_pos_anchor = int(np.random.uniform(0,max_length - self.length+1))\n","        X_anchor = self.data[index][:,start_pos_anchor:start_pos_anchor+self.length]\n","        anchor_wvlt = reference_transform(X_anchor)\n","        y_anchor = self.labels[index]\n","\n","        # create list of possible positive index samples\n","        positive_list = [\n","            i for i in self.index if self.labels[i] == y_anchor and i != index]\n","        positive_index = np.random.choice(positive_list)\n","\n","        max_length = self.data[positive_index].shape[1]\n","        start_pos_positive = int(np.random.uniform(0,max_length - self.length+1))\n","\n","        positive = self.data[positive_index][:,start_pos_positive:start_pos_positive+self.length]\n","        # get the wavelet transform of that sample (the two channels are concatenated along the last dim)\n","        positive_wvlt = reference_transform(positive)\n","        \n","        # same here\n","        negative_list = [\n","            i for i in self.index if self.labels[i] != y_anchor]\n","        negative_index = np.random.choice(negative_list)\n","\n","        max_length = self.data[negative_index].shape[1]\n","        start_pos_negative = int(np.random.uniform(0,max_length - self.length+1))\n","\n","        negative = self.data[negative_index][:,start_pos_negative:start_pos_negative+self.length]\n","        negative_wvlt = reference_transform(negative)\n","\n","        # concatenate the data for the TCN and the haar wavelet transform\n","        # they will be split in the forward pass\n","        return torch.cat((X_anchor, anchor_wvlt),1), \\\n","            torch.cat((positive, positive_wvlt),1), \\\n","            torch.cat((negative, negative_wvlt),1), \\\n","            y_anchor\n","\n","\n","class TestDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Is to be used with Dataloader for testing only\n","    \"\"\"\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        wvlt = reference_transform(x)\n","        out = torch.cat((x,wvlt), 1)\n","        return out, self.labels[index]\n","        \n","\n","class FromFiles:\n","    \"\"\"\n","    Used to load the data from the files and split between test and train sets\n","    \"\"\"\n","    def __init__(self, input_dir, input_length):\n","        self.input_dir = input_dir\n","        self.input_length = input_length\n","\n","        self.data, self.labels = self.__load_dataset()\n","        self.index = [i for i in range(len(self.data))]\n","\n","    def __load_dataset(self):\n","        x=[]\n","        y=[]\n","        for dir,_,files in os.walk(self.input_dir):\n","            for file in files:\n","                label = int(file.split(\"_\")[1])-1\n","                df = pd.read_csv(dir + \"/\" + file, index_col=0)\n","                df = preprocess(df).to_numpy().transpose()\n","                x.append(torch.from_numpy(df).float())\n","                y.append(label)\n","        return x,y\n","\n","\n","    def split_train_test(self):\n","        x_test, y_test = [], []\n","        for label in range(5):\n","            possible_list = [i for i in self.index if self.labels[i] == label]\n","            choosen_index = np.random.choice(possible_list)\n","\n","            positive = self.data[choosen_index]\n","            max_length = positive.shape[1]\n","            train, test = torch.split(positive, [max_length-self.input_length, self.input_length], dim=1)\n","\n","            x_test.append(test)\n","            y_test.append(label)\n","            self.data[choosen_index] = train\n","            \n","        return self.data, self.labels, x_test, y_test\n"]},{"cell_type":"code","execution_count":50,"id":"878cf30f","metadata":{},"outputs":[],"source":["class Dataloader():\n","    \"\"\"Homemade dataloader for our needs in training\n","    This is different from the other one as it allows for \"infinite\" batches, even when the data only has \n","    19 points. When setting bacht_size*number_batches bigger that the total number of points in the dataset,\n","    this dataloader will just loop again from the beginning.\"\"\"\n","\n","    def __init__(self, dataset: TrainDataset, batch_size: int, number_batch: int, shuffle = True):\n","        self.dataset = dataset\n","        self.b_size = batch_size\n","        self.n_batches = number_batch\n","        self.current_batch = 0\n","        self.shuffle = shuffle\n","\n","    def __iter__(self):\n","        self.current_batch = 0\n","        self.index_list = [i%len(self.dataset) for i in range(max(self.b_size,len(self.dataset)))]\n","        if self.shuffle:\n","            np.random.shuffle(self.index_list)\n","        self.i = -1\n","        return self\n","\n","    def __next__(self):\n","        dataset_length = len(self.dataset)\n","        if self.current_batch < dataset_length:\n","            a_out, p_out, n_out, l_out = [], [], [], []\n","            for _ in range(self.b_size):\n","                self.i = (self.i+1)%dataset_length\n","                a, p, n, l = self.dataset[self.index_list[self.i]]\n","                a_out.append(a)\n","                p_out.append(p)\n","                n_out.append(n)\n","                l_out.append(torch.Tensor([l]))\n","            self.current_batch += 1\n","            a_out = torch.stack(a_out)\n","            p_out = torch.stack(p_out)\n","            n_out = torch.stack(n_out)\n","            l_out = torch.stack(l_out)\n","            return a_out, p_out, n_out, l_out\n","        \n","        else:\n","            self.current_batch = 0\n","            raise StopIteration\n"]},{"cell_type":"code","execution_count":51,"id":"02ad9181","metadata":{},"outputs":[{"data":{"text/plain":["Driver2Vec(\n","  (tcn): TemporalConvNet(\n","    (network): Sequential(\n","      (0): TemporalBlock(\n","        (conv1): Conv1d(31, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (net): Sequential(\n","          (0): Conv1d(31, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.1, inplace=False)\n","          (4): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.1, inplace=False)\n","        )\n","        (downsample): Conv1d(31, 32, kernel_size=(1,), stride=(1,))\n","        (relu): ReLU()\n","      )\n","      (1): TemporalBlock(\n","        (conv1): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (net): Sequential(\n","          (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.1, inplace=False)\n","          (4): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.1, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","    )\n","  )\n","  (haar): WaveletPart(\n","    (fc1): Linear(in_features=6200, out_features=15, bias=True)\n","    (fc2): Linear(in_features=6200, out_features=15, bias=True)\n","  )\n","  (input_bn): LayerNorm((62,), eps=1e-05, elementwise_affine=True)\n","  (linear): Linear(in_features=62, out_features=62, bias=True)\n","  (activation): Sigmoid()\n",")"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["input_channels = 31\n","input_length = 400\n","channel_sizes = [32,32]\n","output_size = 62\n","kernel_size = 16\n","dropout = 0.1\n","model = Driver2Vec(input_channels, input_length, channel_sizes, output_size, kernel_size=kernel_size, dropout=dropout, do_wavelet=True)\n","model.to(device)"]},{"cell_type":"code","execution_count":52,"id":"3692f2fb","metadata":{},"outputs":[],"source":["\n","# # datasets parameters\n","# params = {'batch_size': 4,\n","#           'shuffle': True,\n","#           'num_workers': 1}\n","\n","\n","fromfiles= FromFiles(\"./dataset\", input_length)\n","x_train, y_train, x_test, y_text = fromfiles.split_train_test()\n","training_set = TrainDataset(x_train, y_train, input_length)\n","training_generator = Dataloader(training_set, 5, 8)\n","\n","loss = nn.TripletMarginLoss(margin=0.5)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"]},{"cell_type":"code","execution_count":53,"id":"ec33b00d","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3aed396cc194649ac148266316515c4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["epochs = 50\n","\n","model.train()\n","for epoch in (pbar := tqdm(range(epochs))):\n","    loss_list = []\n","    for anchor, positive, negative, label in training_generator:\n","        anchor = anchor.to(device)\n","        positive = positive.to(device)\n","        negative = negative.to(device)\n","        \n","        optimizer.zero_grad()\n","\n","        y_positive = model(positive)\n","        y_negative = model(negative)\n","\n","        y_anchor = model(anchor)\n","\n","        loss_value = loss(y_anchor, y_positive, y_negative)\n","        Dap = (y_anchor-y_positive).norm(2)\n","        Dan = (y_anchor-y_negative).norm(2)\n","        loss_value.backward()\n","\n","        optimizer.step()\n","\n","        loss_list.append(loss_value.cpu().detach().numpy())\n","    #print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(loss_list)))\n","    pbar.set_description(\"Loss: %0.5g, Dap: %0.3g, Dan: %0.3g, Epochs\" % (np.mean(loss_list), Dap, Dan))\n"]},{"cell_type":"code","execution_count":54,"id":"517b5b7a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000094 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["/home/wslachille/.local/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_trees` in params. Will use it instead of argument\n","  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"]}],"source":["import lightgbm as lgbm\n","\n","params1 = {'batch_size': 19,\n","          'shuffle': False,\n","          'num_workers': 2}\n","\n","params2 = {'batch_size': 38,\n","          'shuffle': False,\n","          'number_batch': 1}\n","\n","classifier_train_set = TrainDataset(x_test, y_text, input_length)\n","classifier_train_generator = DataLoader(training_set, **params1)\n","\n","x_train_classifier = []\n","y_train_classifier = []\n","model.train(False)\n","for data,_,_, label in classifier_train_generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","\n","for i in range(5):\n","    x_train_classifier.append(embed[i,:].cpu().detach().numpy().squeeze())\n","    y_train_classifier.append(int(label[i]))\n","\n","x_train_classifier = np.array(x_train_classifier)\n","y_train_classifier = np.array(y_train_classifier)\n","\n","lgb_train = lgbm.Dataset(x_train_classifier, y_train_classifier)\n","\n","params = {\n","    'boosting_type': 'gbdt',\n","    'objective': 'multiclass',\n","    'num_class':5,\n","    'metric': 'multi_logloss',\n","    'num_leaves': 32,\n","    'feature_fraction': 0.8,\n","    'bagging_fraction': 0.9,\n","    'max_depth': 8,\n","    'num_trees': 30,\n","    'verbose': 0,\n","    'min_data_in_leaf': 2 # May need to change that with a real test set\n","}\n","\n","clf = lgbm.train(params,lgb_train)"]},{"cell_type":"code","execution_count":55,"id":"886d08d6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[9.73424738e-01 2.65752618e-02 3.89600569e-16 3.89600569e-16\n","  3.89600569e-16]\n"," [9.74950312e-01 2.50496875e-02 4.92833643e-16 4.92833643e-16\n","  4.92833643e-16]\n"," [6.33962522e-01 3.66037478e-01 1.47567342e-15 1.47567342e-15\n","  1.47567342e-15]\n"," [9.74044636e-01 2.59553642e-02 3.32180997e-16 3.32180997e-16\n","  3.32180997e-16]\n"," [6.94489151e-01 3.05510849e-01 1.61426021e-15 1.61426021e-15\n","  1.61426021e-15]] [0, 1, 2, 3, 4]\n"]}],"source":["params = {'batch_size': 5,\n","          'shuffle': False,\n","          'num_workers': 1}\n","\n","classifier_test_set = TestDataset(x_test, y_text)\n","classifier_test_generator = DataLoader(classifier_test_set, **params)\n","\n","x_test_classifier = []\n","y_test_classifier = []\n","for data, label in classifier_test_generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","\n","for i in range(5):\n","    x_test_classifier.append(embed[i,:].cpu().detach().numpy().squeeze())\n","    y_test_classifier.append(int(label[i]))\n","\n","\n","y_pred = clf.predict(x_test_classifier)\n","print(y_pred, y_test_classifier)"]},{"cell_type":"code","execution_count":56,"id":"bd49df41","metadata":{},"outputs":[],"source":["def distance_matrix(tensor: torch.Tensor):\n","    \"\"\"\n","    computes the distance matrix between each point of the input Tensor\n","    tensor should have dimension (L,D) where D is the dimension of the vectors\"\"\"\n","\n","    res= torch.cdist(tensor, tensor)\n","    return res\n"]},{"cell_type":"code","execution_count":57,"id":"b36d8ce5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[9.38524115e-02 9.06147589e-01 7.49920027e-16 7.49920027e-16\n","  7.49920027e-16]\n"," [9.74950312e-01 2.50496875e-02 4.92833643e-16 4.92833643e-16\n","  4.92833643e-16]\n"," [9.73424738e-01 2.65752618e-02 3.89600569e-16 3.89600569e-16\n","  3.89600569e-16]\n"," [9.98786595e-01 1.21340488e-03 8.68122681e-17 8.68122681e-17\n","  8.68122681e-17]\n"," [9.74044636e-01 2.59553642e-02 3.32180997e-16 3.32180997e-16\n","  3.32180997e-16]\n"," [9.74950312e-01 2.50496875e-02 4.92833643e-16 4.92833643e-16\n","  4.92833643e-16]\n"," [9.73424738e-01 2.65752618e-02 3.89600569e-16 3.89600569e-16\n","  3.89600569e-16]\n"," [9.98786595e-01 1.21340488e-03 8.68122681e-17 8.68122681e-17\n","  8.68122681e-17]\n"," [9.74044636e-01 2.59553642e-02 3.32180997e-16 3.32180997e-16\n","  3.32180997e-16]\n"," [9.74950312e-01 2.50496875e-02 4.92833643e-16 4.92833643e-16\n","  4.92833643e-16]\n"," [9.98786595e-01 1.21340488e-03 8.68122681e-17 8.68122681e-17\n","  8.68122681e-17]\n"," [9.74044636e-01 2.59553642e-02 3.32180997e-16 3.32180997e-16\n","  3.32180997e-16]\n"," [9.74950312e-01 2.50496875e-02 4.92833643e-16 4.92833643e-16\n","  4.92833643e-16]\n"," [9.80089134e-01 1.99108656e-02 2.92426562e-16 2.92426562e-16\n","  2.92426562e-16]\n"," [9.98786595e-01 1.21340488e-03 8.68122681e-17 8.68122681e-17\n","  8.68122681e-17]\n"," [7.31840274e-02 9.26815973e-01 6.68390861e-16 6.68390861e-16\n","  6.68390861e-16]\n"," [9.74950312e-01 2.50496875e-02 4.92833643e-16 4.92833643e-16\n","  4.92833643e-16]\n"," [9.73424738e-01 2.65752618e-02 3.89600569e-16 3.89600569e-16\n","  3.89600569e-16]\n"," [9.98786595e-01 1.21340488e-03 8.68122681e-17 8.68122681e-17\n","  8.68122681e-17]] [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]\n"]}],"source":["params = {\"batch_size\": 19,\n","        \"shuffle\": False,\n","        \"num_workers\": 1}\n","\n","generator = DataLoader(training_set, **params)\n","x_test_classifier = []\n","y_test_classifier = []\n","for data,_,_, label in generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","\n","for i in range(19):\n","    x_test_classifier.append(embed[i,:].cpu().detach().numpy().squeeze())\n","    y_test_classifier.append(int(label[i]))\n","\n","\n","y_pred = clf.predict(x_test_classifier)\n","print(y_pred, y_test_classifier)"]},{"cell_type":"code","execution_count":58,"id":"6d9506cd","metadata":{},"outputs":[],"source":["from tsne_torch import TorchTSNE as TSNE\n","import matplotlib.pyplot as plt\n","\n","params = {'batch_size': 60,\n","          'shuffle': False,\n","          'number_batch': 1}\n","\n","generator = Dataloader(training_set, **params)\n","x_tsne = []\n","y_tsne = []\n","for data,_,_, label in generator:\n","    data = data.to(device)\n","    embed = model(data)\n","\n","\n","\n","for i in range(embed.shape[0]):\n","    x_tsne.append(embed[i,:].cpu().detach().numpy().squeeze())\n","    y_tsne.append(int(label[i]))\n"]},{"cell_type":"code","execution_count":59,"id":"b3b6c646","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["using cuda\n","initializing...\n"]},{"ename":"IndexError","evalue":"index 62 is out of bounds for dimension 1 with size 62","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/mnt/e/Cours2A/Deep learning/Driver2Vec/tcn-testing.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/Cours2A/Deep%20learning/Driver2Vec/tcn-testing.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m X_emb \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, perplexity\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mfit_transform(embed)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py:256\u001b[0m, in \u001b[0;36mTorchTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=247'>248</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=248'>249</a>\u001b[0m \u001b[39mLearns the t-stochastic neighbor embedding of the given data.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=249'>250</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=252'>253</a>\u001b[0m \u001b[39m:return: ndarray (n_samples, n_components)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=253'>254</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=254'>255</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=255'>256</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _tsne(\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=256'>257</a>\u001b[0m         X,\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=257'>258</a>\u001b[0m         no_dims\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_components,\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=258'>259</a>\u001b[0m         initial_dims\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitial_dims,\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=259'>260</a>\u001b[0m         perplexity\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperplexity,\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=260'>261</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=261'>262</a>\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=262'>263</a>\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py:158\u001b[0m, in \u001b[0;36m_tsne\u001b[0;34m(X, no_dims, initial_dims, perplexity, max_iter, verbose)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=155'>156</a>\u001b[0m \u001b[39m# Initialize variables\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=156'>157</a>\u001b[0m \u001b[39mif\u001b[39;00m initial_dims \u001b[39m<\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=157'>158</a>\u001b[0m     X \u001b[39m=\u001b[39m _pca_torch(X, initial_dims)\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=158'>159</a>\u001b[0m \u001b[39melif\u001b[39;00m verbose:\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=159'>160</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mskipping PCA because initial_dims is larger than input dimensionality\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py:128\u001b[0m, in \u001b[0;36m_pca_torch\u001b[0;34m(X, no_dims)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=125'>126</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(d):\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=126'>127</a>\u001b[0m     \u001b[39mif\u001b[39;00m l[i, \u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=127'>128</a>\u001b[0m         M[:, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m M[:, i]\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=128'>129</a>\u001b[0m         i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/wslachille/.local/lib/python3.8/site-packages/tsne_torch/tsne_torch.py?line=130'>131</a>\u001b[0m Y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(X, M[:, \u001b[39m0\u001b[39m:no_dims])\n","\u001b[0;31mIndexError\u001b[0m: index 62 is out of bounds for dimension 1 with size 62"]}],"source":["\n","X_emb = TSNE(n_components=2, perplexity=80, n_iter=10000, verbose=True).fit_transform(embed)\n"]},{"cell_type":"code","execution_count":null,"id":"d3128ad7","metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","plt.scatter(X_emb[:,0], X_emb[:,1], marker=\"+\", c=y_tsne)\n","plt.show()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"tcn-testing.ipynb","provenance":[{"file_id":"https://github.com/AchilleBailly/Driver2Vec/blob/kick-off/tcn-testing.ipynb","timestamp":1647516778114}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
